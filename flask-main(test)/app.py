from flask import Flask, send_from_directory
from flask import Flask, render_template, request, jsonify, Response
import openai
from nltk.tokenize import sent_tokenize
from flask_cors import CORS
import json
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np
import re
import os
from datetime import datetime

app = Flask(__name__, static_folder="static")
CORS(app)
# ğŸ”¹ ë°©ë¬¸ì ë°ì´í„° ì €ì¥ íŒŒì¼
VISITOR_FILE = 'visitors.json'

# ğŸ”¹ ë°©ë¬¸ì ìˆ˜ ë¡œë“œ í•¨ìˆ˜
def load_visitors():
    if os.path.exists(VISITOR_FILE):
        with open(VISITOR_FILE, 'r', encoding='utf-8') as file:
            return json.load(file)
    return {"today": 0, "total": 0, "last_date": "", "ips": []}

# ğŸ”¹ ë°©ë¬¸ì ìˆ˜ ì €ì¥ í•¨ìˆ˜
def save_visitors(data):
    with open(VISITOR_FILE, 'w', encoding='utf-8') as file:
        json.dump(data, file, ensure_ascii=False, indent=4)

# ğŸ”¹ ë°©ë¬¸ì ìˆ˜ API ì—”ë“œí¬ì¸íŠ¸ (ì¤‘ë³µ ë°©ë¬¸ ë°©ì§€ ì¶”ê°€)
@app.route('/visitors', methods=['GET'])
def get_visitors():
    visitors = load_visitors()
    today_date = datetime.now().strftime('%Y-%m-%d')

    # ğŸ”¹ ì‹¤ì œ í´ë¼ì´ì–¸íŠ¸ IP ê°€ì ¸ì˜¤ê¸°
    if request.headers.get("X-Forwarded-For"):
        user_ip = request.headers.get("X-Forwarded-For").split(',')[0]
    else:
        user_ip = request.remote_addr

    print(f"ğŸ”¹ Detected IP: {user_ip}")  # ë””ë²„ê¹… ë¡œê·¸

    # ğŸ”¹ ë‚ ì§œê°€ ë³€ê²½ë˜ë©´ today ë°©ë¬¸ì ìˆ˜ ì´ˆê¸°í™”
    if visitors["last_date"] != today_date:
        visitors["last_date"] = today_date
        visitors["today"] = 0
        visitors["ips"] = []  # IP ëª©ë¡ ì´ˆê¸°í™”

    # ğŸ”¹ ì¤‘ë³µ ë°©ë¬¸ ë°©ì§€: ê°™ì€ IPì—ì„œ ìƒˆë¡œê³ ì¹¨í•´ë„ ë°©ë¬¸ì ìˆ˜ ì¦ê°€ X
    if user_ip not in visitors["ips"]:
        visitors["ips"].append(user_ip)
        visitors["today"] += 1
        visitors["total"] += 1
        save_visitors(visitors)

    return jsonify({"today": visitors["today"], "total": visitors["total"]})

openai.api_key = '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'

with open('data.txt', 'r', encoding='utf-8') as file:
    text = file.read()

def find_relevant_parts(text, user_question):
    keywords = user_question.lower().split()
    text_parts = text.lower().split('content_box')  # Split by ';'

    selected_parts = []

    # For the first word
    if len(keywords) > 0:
        first_word = keywords[0]
        for length in [3, 4, 5]:  # Prefix lengths (3, 4, 5)
            first_word_prefix = first_word[:length]  # Prefix of the first word
            prefix_parts = [part for part in text_parts if any(word.startswith(first_word_prefix) for word in part.split()) and part not in selected_parts]
            selected_parts.extend(prefix_parts[:3])

    # For the second word
    if len(keywords) > 1:
        second_word = keywords[1]
        for length in [2, 3]:  # Prefix lengths (2, 3)
            second_word_prefix = second_word[:length]
            prefix_parts = [part for part in text_parts if any(word.startswith(second_word_prefix) for word in part.split()) and part not in selected_parts]
            selected_parts.extend(prefix_parts[:3])

        # First and Second word combined prefix
        for part in text_parts:
            if first_word[:2] in part and second_word[:2] in part and part not in selected_parts:
                selected_parts.append(part)
            if len(selected_parts) >= 4:  # Max of 7 parts with the current criteria
                break

    # For the third word
    if len(keywords) > 2:
        third_word = keywords[2]
        for length in [2, 3]:  # Prefix lengths (2, 3)
            third_word_prefix = third_word[:length]
            prefix_parts = [part for part in text_parts if any(word.startswith(third_word_prefix) for word in part.split()) and part not in selected_parts]
            selected_parts.extend(prefix_parts[:1])
            
        # Second and Third word combined prefix
        for part in text_parts:
            if second_word[:2] in part and third_word[:2] in part and part not in selected_parts:
                selected_parts.append(part)
            if len(selected_parts) >= 6:  # Max of 10 parts with the current criteria
                break

    # For the fourth word
    if len(keywords) > 3:
        fourth_word = keywords[3]
        # Third and Fourth word combined prefix
        for part in text_parts:
            if third_word[:2] in part and fourth_word[:2] in part and part not in selected_parts:
                selected_parts.append(part)
            if len(selected_parts) >= 8:  # Max of 12 parts with the current criteria
                break

    return selected_parts[:2]

def split_text_into_chunks(text, chunk_size=1000, overlap=200):
    words = text.split()
    chunks = []
    start = 0

    while start < len(words):
        end = start + chunk_size
        chunks.append(' '.join(words[start:end]))
        start += (chunk_size - overlap)

    return chunks

document_sentences = split_text_into_chunks(text)

def preprocess(text):
    return ' '.join(text.split())

def find_most_similar_sentences(user_question, document_sentences, top_n=10):
    sentences = [preprocess(sentence) for sentence in document_sentences]
    input_sentence = preprocess(user_question)
    vectorizer = TfidfVectorizer().fit_transform([user_question] + sentences)
    cosine_matrix = cosine_similarity(vectorizer[0:1], vectorizer[1:])
    similar_sentences = np.argsort(cosine_matrix[0])[-top_n:][::-1]
    return [sentences[i] for i in similar_sentences]

def convert_urls_to_links(text):
    url_pattern = re.compile(r'(http[s]?://\S+)')
    return url_pattern.sub(r'<a href="\1" target="_blank">\1</a>', text)
@app.route('/static/<path:filename>')
def serve_static(filename):
    return send_from_directory(app.static_folder, filename)

@app.route("/")
def index():
    return render_template('./index.html')

@app.route('/chat', methods=['POST'])
def chat():
    data = request.get_json()
    user_question = data.get('question') + "?"

    keywords = ["ë¼í—¬", "ì˜¤ë‘ìš°íƒ„"]
    if any(keyword in user_question for keyword in keywords):
        answer = "ì´ê²ƒì€ í•˜ì´í¼ë§í¬ë¥¼ í¬í•¨í•œ ë‹µë³€ì…ë‹ˆë‹¤. <a href='https://naver.com'>ì—¬ê¸°</a>ë¥¼ í´ë¦­í•˜ì„¸ìš”."
        response = Response(response=json.dumps({"answer": answer}, ensure_ascii=False),
                            status=200,
                            mimetype="application/json")
        return response

    relevant_text = find_most_similar_sentences(user_question, document_sentences, top_n=10)

    system_message = f"{relevant_text}\n\n ë‹¹ì‹ ì€ ì²­ë ´ì±—ë´‡ í”„ë½ì‹œìŠ¤ì…ë‹ˆë‹¤. 1. ì•ì˜ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ìµœëŒ€ 7ë¬¸ì¥ ì´ë‚´+70ë‹¨ì–´ ì´ë‚´ë¡œ ìš”ì•½í•´ì„œ ì¡´ëŒ“ë§ë¡œ ë‹µí•´ì¤˜. 2. ì§ˆë¬¸ì´ ë‚´ìš©ê³¼ ê´€ê³„ì—†ìœ¼ë©´ ë‹¤ì‹œ ì§ˆë¬¸í•´ì£¼ì‹œê² ì–´ìš”? ë¼ê³  ë‹µë³€í•´ì¤˜ 3.ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œë§Œ ë‹µí•´ì¤˜.ë˜í•œ ì˜ˆì™¸ ì‚¬í•­ì´ë‚˜ ì‚¬ë¡€ê°€ ìˆìœ¼ë©´ ê·¸ê²ƒë„ ë‹µí•´ì¤˜ 4. ì‚¬ìš©ìì—ê²Œ ì¬ì§ˆë¬¸ ê¸ˆì§€ 5. ê´€ë ¨ ë²•ë ¹ë„ ê°™ì´ ë‹µë³€(ë‹µë³€ì‹œ ì°¸ì¡°í•œ ë¬¸ì¥ê³¼ ì •í™•íˆ ê´€ë ¨ëœ ë²•ë ¹) 6. ë§í¬ê°€ ìˆìœ¼ë©´ ë§í¬ë„ ë‹µë³€(ë‹µë³€ê³¼ ê´€ë ¨ìˆëŠ” ë§í¬ë§Œ) 7. ë‹µë³€ê³¼ ê´€ë ¨ìˆëŠ” ë§í¬ë€: ì§ˆë¬¸ê³¼ ë‹µë³€ ì²­í¬ëŠ” hideë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë‚˜ë‰˜ì–´ì§„ë‹¤. ê´€ë ¨ ì—†ëŠ” ë§í¬ëŠ” í¬í•¨ì‹œí‚¤ë©´ ì•ˆë©ë‹ˆë‹¤."

    response = openai.ChatCompletion.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "system", "content": system_message},
            {"role": "user", "content": user_question}
        ]
    )
    answer_with_links = convert_urls_to_links(response['choices'][0]['message']['content'])

    return jsonify(answer=answer_with_links)

if __name__ == "__main__":
    app.run(host="0.0.0.0", port=5000)
